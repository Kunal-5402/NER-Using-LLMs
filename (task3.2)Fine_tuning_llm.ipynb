{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your custom dataset\n",
    "custom_dataset = pd.read_csv('ner_data.conll', sep='\\t', header=None, names=['token', 'ner_label'])\n",
    "\n",
    "# Split dataset into train, validation, and test sets\n",
    "train_data, test_data = train_test_split(custom_dataset, test_size=0.2, random_state=42)\n",
    "train_data, validation_data = train_test_split(train_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(custom_dataset['ner_label'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset sample:\n",
      "         token ner_label\n",
      "2492     owner         O\n",
      "513        NaN         O\n",
      "1634      risk         O\n",
      "3361   dometic         O\n",
      "1758  increase         O\n",
      "\n",
      "Eval dataset sample:\n",
      "            token ner_label\n",
      "1746   electrical         O\n",
      "2315        crash         O\n",
      "1293     position         O\n",
      "2205  compartment         O\n",
      "2943     movement         O\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset sample:\")\n",
    "print(train_data.head())\n",
    "\n",
    "print(\"\\nEval dataset sample:\")\n",
    "print(validation_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset sample after dropping NaN values:\n",
      "         token  ner_label\n",
      "2492     owner          O\n",
      "1634      risk          O\n",
      "3361   dometic          O\n",
      "1758  increase          O\n",
      "1399  national  I-company\n",
      "\n",
      "Eval dataset sample after dropping NaN values:\n",
      "            token ner_label\n",
      "1746   electrical         O\n",
      "2315        crash         O\n",
      "1293     position         O\n",
      "2205  compartment         O\n",
      "2943     movement         O\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with NaN values\n",
    "train_data.dropna(inplace=True)\n",
    "validation_data.dropna(inplace=True)\n",
    "\n",
    "# Print the updated dataset sample\n",
    "print(\"Train dataset sample after dropping NaN values:\")\n",
    "print(train_data.head())\n",
    "\n",
    "print(\"\\nEval dataset sample after dropping NaN values:\")\n",
    "print(validation_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mapping of named entities to numerical labels\n",
    "label_map = {\n",
    "    \"O\": 0,                # 'O' represents tokens outside of named entities\n",
    "    \"B-company\": 1,        # 'B-' represents the beginning of a company entity\n",
    "    \"I-company\": 2,        # 'I-' represents tokens inside a company entity\n",
    "    \"B-failure issue\": 3,\n",
    "    \"I-failure issue\": 4,\n",
    "    \"B-corrective action\": 5,\n",
    "    \"I-corrective action\": 6,\n",
    "    \"B-vehicle model\": 7,\n",
    "    \"I-vehicle model\": 8,\n",
    "    \"B-contact\": 9,\n",
    "    \"I-contact\": 10,\n",
    "    \"B-standard\": 11,\n",
    "    \"I-standard\": 12,\n",
    "    \"I-component\": 13,     # Include 'I-component' in the label mapping\n",
    "    \"B-component\": 14     # Include 'I-component' in the label mapping\n",
    "}\n",
    "\n",
    "# Modify tokenize_text function to use numerical labels\n",
    "def tokenize_text(text, label):\n",
    "    tokenized_input = tokenizer(text, truncation=True, padding='max_length', max_length=200, return_tensors='pt')\n",
    "    return {\n",
    "        'input_ids': tokenized_input['input_ids'],\n",
    "        'attention_mask': tokenized_input['attention_mask'],\n",
    "        'token_type_ids': tokenized_input.get('token_type_ids', None),  \n",
    "        'labels': torch.tensor(label_map[label])  # Convert string label to numerical format\n",
    "    }\n",
    "\n",
    "# Preprocess training and validation data\n",
    "train_features = [(tokenize_text(text, label),) for text, label in zip(train_data['token'], train_data['ner_label'])]\n",
    "validation_features = [(tokenize_text(text, label),) for text, label in zip(validation_data['token'], validation_data['ner_label'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = []\n",
    "for text, label in zip(train_data['token'], train_data['ner_label']):\n",
    "    feature = tokenize_text(text, label)\n",
    "    train_features.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = []\n",
    "for text, label in zip(validation_data['token'], validation_data['ner_label']):\n",
    "    feature = tokenize_text(text, label)\n",
    "    validation_features.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
